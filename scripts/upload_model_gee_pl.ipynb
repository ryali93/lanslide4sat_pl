{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import h5py\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Sequence' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/ryali93/Desktop/l4s/scripts/upload_model_gee_pl.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryali93/Desktop/l4s/scripts/upload_model_gee_pl.ipynb#W1sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_minmax\u001b[39m(data):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryali93/Desktop/l4s/scripts/upload_model_gee_pl.ipynb#W1sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (data \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(data)) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mmax(data) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(data))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ryali93/Desktop/l4s/scripts/upload_model_gee_pl.ipynb#W1sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDataGenerator\u001b[39;00m(Sequence):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryali93/Desktop/l4s/scripts/upload_model_gee_pl.ipynb#W1sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, all_train, all_mask, norm_dicc, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ryali93/Desktop/l4s/scripts/upload_model_gee_pl.ipynb#W1sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_train \u001b[39m=\u001b[39m all_train\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequence' is not defined"
          ]
        }
      ],
      "source": [
        "norm_dicc = {\n",
        "    \"l4s\": {\n",
        "        \"B1\": [0.0, 3.1057398816745447],\n",
        "        \"B2\": [0.0, 19.76154895369598],\n",
        "        \"B3\": [0.0, 31.551829580344585],\n",
        "        \"B4\": [0.0, 33.16014987560649],\n",
        "        \"B5\": [0.0, 9.972775876378055],\n",
        "        \"B6\": [0.0, 4.144120061411549],\n",
        "        \"B7\": [0.0, 3.6925308568047677],\n",
        "        \"B8\": [0.0, 8.312890666119067],\n",
        "        \"B9\": [0.0, 3.5485627183025263],\n",
        "        \"B10\": [0.0, 21.44150144339325],\n",
        "        \"B11\": [0.0, 5.786441765686274],\n",
        "        \"B12\": [0.0, 19.661322873426624],\n",
        "        \"B13\": [0.0, 4.040945090994509],\n",
        "        \"B14\": [0.0, 5.121427202344607]\n",
        "    },\n",
        "    \"own\": {\n",
        "        \"B1\": [681.0, 3165.0],\n",
        "        \"B2\": [183.0, 8655.0],\n",
        "        \"B3\": [258.0, 7697.0],\n",
        "        \"B4\": [68.0, 6779.0],\n",
        "        \"B5\": [1.0, 5293.0],\n",
        "        \"B6\": [0.0, 5600.0],\n",
        "        \"B7\": [58.0, 6544.0],\n",
        "        \"B8\": [111.0, 8930.0],\n",
        "        \"B9\": [36.0, 2972.0],\n",
        "        \"B10\": [1.0, 270.0],\n",
        "        \"B11\": [36.0, 7280.0],\n",
        "        \"B12\": [1.0, 12937.0],\n",
        "        \"B13\": [0.0, 1.4837092161178589],\n",
        "        \"B14\": [0.000001, 4128.0]\n",
        "    }\n",
        "}\n",
        "def read_data(path, test_size, seed):\n",
        "    # TRAIN_PATH = f\"{path}/img/*.h5\" # data_val\n",
        "    # TRAIN_MASK = f'{path}/mask/*.h5' # data_val\n",
        "    # all_train = sorted(glob.glob(TRAIN_PATH))\n",
        "    # all_mask = sorted(glob.glob(TRAIN_MASK))\n",
        "    # train_size = int(0.8 * len(dataset))\n",
        "    # val_size = len(dataset) - train_size\n",
        "    # train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    # x_train, x_valid, y_train, y_valid = train_test_split(all_train, all_mask, test_size=test_size, shuffle=True, random_state=seed)\n",
        "    # return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "    from dataset import DatasetLandslide\n",
        "    data_path = '/home/tidop/Desktop/projects/l4s/data/TrainData'\n",
        "    dataset = DatasetLandslide(data_path)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "def normalize_minmax(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, all_train, all_mask, norm_dicc, batch_size=16):\n",
        "        self.all_train = all_train\n",
        "        self.all_mask = all_mask\n",
        "        self.batch_size = batch_size\n",
        "        self.n_imgs = len(all_train)\n",
        "        self.norm = norm_dicc\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(self.n_imgs / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.all_train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.all_mask[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        \n",
        "        X = np.zeros((self.batch_size, 128, 128, 6))\n",
        "        Y = np.zeros((self.batch_size, 128, 128, 1))\n",
        "        \n",
        "        for i, (img, mask) in enumerate(zip(batch_x, batch_y)):\n",
        "            with h5py.File(img) as hdf:\n",
        "\n",
        "                dict_norm = self.norm[\"l4s\"] if \"image\" in img else self.norm[\"own\"]\n",
        "                \n",
        "                data = np.array(hdf.get('img'))\n",
        "\n",
        "                # assign 0 for the nan value\n",
        "                data[np.isnan(data)] = 0.000001\n",
        "\n",
        "                # ndvi calculation\n",
        "                data_red = data[:, :, 3]\n",
        "                data_nir = data[:, :, 7]\n",
        "                data_ndvi = np.divide(data_nir - data_red, np.add(data_nir, data_red))\n",
        "                \n",
        "                # final array\n",
        "                X[i, :, :, 0] = normalize_minmax(data[:, :, 1]) # , dict_norm[\"B2\"]\n",
        "                X[i, :, :, 1] = normalize_minmax(data[:, :, 2]) # , dict_norm[\"B3\"]\n",
        "                X[i, :, :, 2] = normalize_minmax(data[:, :, 3]) # , dict_norm[\"B4\"]\n",
        "                X[i, :, :, 3] = normalize_minmax(data[:, :, 12]) # , dict_norm[\"B13\"]\n",
        "                X[i, :, :, 4] = normalize_minmax(data[:, :, 13]) # , dict_norm[\"B14\"]\n",
        "                X[i, :, :, 5] = data_ndvi\n",
        "\n",
        "\n",
        "            with h5py.File(mask) as hdf:\n",
        "                data=np.array(hdf.get('mask'))\n",
        "                Y[i, :, :, 0] = data\n",
        "\n",
        "        X[np.isnan(X)] = 0.000001\n",
        "        return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-10 10:16:29.835997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9539 MB memory:  -> device: 0, name: NVIDIA RTX A2000 12GB, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "with open(\"../models/model_resnet34_6b_ns.json\", \"r\") as json_file:\n",
        "    json_config = json_file.read()\n",
        "model = tf.keras.models.model_from_json(json_config)\n",
        "model.load_weights(\"../models/model_resnet34_6b_ns.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "path_dataset = \"../data/TrainData\"\n",
        "p_x_train, p_x_valid, p_y_train, p_y_valid = read_data(path_dataset, 0.3, SEED)\n",
        "valid_generator = DataGenerator(p_x_valid, p_y_valid, norm_dicc, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_valid = valid_generator.__getitem__(0)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16, 128, 128, 6)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "pred_img = model.predict(x_valid)\n",
        "pred_img = (pred_img > threshold).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Ver img { run: \"auto\" }\n",
        "img = 12 #@param {type:\"slider\", min:0, max:113, step:1}\n",
        "fig,(ax1,ax3)= plt.subplots(1,2,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "# ax2.imshow(y_valid[img, :, :, 0])\n",
        "# ax2.set_title(\"Label\")\n",
        "ax3.imshow(x_valid[img, :, :, 1:4])\n",
        "ax3.set_title('Training Image')\n",
        "# plt.savefig(f'graph/outputs/mod_564_6b/point_{img}.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GEE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ee\n",
        "import geemap\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "xmin,ymin,xmax,ymax = [-76.27886, -10.522948, -76.07424, -10.357505]\n",
        "pol = ee.Geometry.Rectangle([xmin,ymin,xmax,ymax])\n",
        "\n",
        "uriBase = 'gs://rgee_dev/COG/'\n",
        "collection = ee.ImageCollection(ee.List([\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26505_FBS_F6970_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26505_FBS_F7000_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26505_FBS_F6960_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26505_FBS_F6990_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26505_FBS_F6980_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F7000_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F6990_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26505_FBS_F6950_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F7010_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F7020_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F6980_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_24988_FBD_F7010_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F6970_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26082_FBS_F6970_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26257_FBS_F6970_RT1.cog.tif'),\n",
        "    ee.Image.loadGeoTIFF(uriBase + 'AP_26257_FBS_F6960_RT1.cog.tif')\n",
        "]))\n",
        "\n",
        "dem = collection.filterBounds(pol).first().clip(pol).select(\"B0\").clip(pol).toFloat().rename(\"B14\")\n",
        "slope = ee.Terrain.slope(dem).select(\"slope\").rename(\"B13\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "RGB_bands = ['B4','B3','B2'] #RGB\n",
        "NDVI_bands = ['B8','B4']\n",
        "\n",
        "d_s2 = ee.ImageCollection('COPERNICUS/S2_SR')\\\n",
        "              .filterDate('2020-06-01', '2020-07-30')\\\n",
        "              .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\\\n",
        "              .filterBounds(pol)\\\n",
        "              .first()\\\n",
        "              .select(\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B7\",\"B8\",\"B8A\",\"B9\",\"B11\",\"B12\")\\\n",
        "              .clip(pol)\n",
        "ndvi = d_s2.normalizedDifference(['B8','B4']).rename(\"B15\")\n",
        "dataset = d_s2.addBands(slope).addBands(dem).addBands(ndvi)\n",
        "\n",
        "def normalize_minmax(data, band, norm_min, norm_max):\n",
        "    band_img = data.expression(\n",
        "        f'(b(\"{band}\") - {norm_min})/({norm_max} - {norm_min})', {\n",
        "            band: data.select(band)\n",
        "        }).rename(band).toFloat().select(band)\n",
        "    return band_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "band2 = normalize_minmax(dataset, \"B2\", norm_dicc[\"own\"][\"B2\"][0], norm_dicc[\"own\"][\"B2\"][1])\n",
        "band3 = normalize_minmax(dataset, \"B3\", norm_dicc[\"own\"][\"B3\"][0], norm_dicc[\"own\"][\"B3\"][1])\n",
        "band4 = normalize_minmax(dataset, \"B4\", norm_dicc[\"own\"][\"B4\"][0], norm_dicc[\"own\"][\"B4\"][1])\n",
        "band13 = normalize_minmax(dataset, \"B13\", norm_dicc[\"own\"][\"B13\"][0], norm_dicc[\"own\"][\"B13\"][1])\n",
        "band14 = normalize_minmax(dataset, \"B14\", norm_dicc[\"own\"][\"B14\"][0], norm_dicc[\"own\"][\"B14\"][1])\n",
        "\n",
        "dataset_img = band2.addBands(band3).addBands(band4).addBands(band13).addBands(band14).addBands(ndvi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map = geemap.Map()\n",
        "# Map.centerObject(pol, 12)\n",
        "# Map.addLayer(dataset_img.select(\"B14\"), {'min': 0, 'max': 1}, 'dem')\n",
        "# Map.addLayer(dataset_img.select(\"B15\"), {'min': -0.2, 'max': 1}, 'NDVI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map = geemap.Map()\n",
        "# Map.centerObject(pol, 12)\n",
        "# Map.addLayer(dataset.select(\"B14\").clip(pol), {'min': 0, 'max': 4000}, 'dem')\n",
        "# Map.addLayer(dataset.select(\"B15\").clip(pol), {'min': -0.2, 'max': 1}, 'NDVI')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputBucket = 'rgee_dev'\n",
        "imageFilePrefix = 'tesis/ld'\n",
        "# Specify patch and file dimensions.\n",
        "imageExportFormatOptions = {\n",
        "  'patchDimensions': [128, 128],\n",
        "  'compressed': True\n",
        "}\n",
        "# Setup the task.\n",
        "imageTask = ee.batch.Export.image.toCloudStorage(\n",
        "  image=dataset_img,\n",
        "  description='Image Export',\n",
        "  fileNamePrefix=imageFilePrefix,\n",
        "  bucket=outputBucket,\n",
        "  scale=10,\n",
        "  fileFormat='TFRecord',\n",
        "  region=pol.buffer(100).getInfo()['coordinates'],\n",
        "  formatOptions=imageExportFormatOptions,\n",
        ")\n",
        "imageTask.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n",
            "Polling for task (id: OFCU652MEA77UJAP4XP7W7P3).\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "while imageTask.active():\n",
        "  print('Polling for task (id: {}).'.format(imageTask.id))\n",
        "  time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "filesList = !gsutil ls 'gs://'{outputBucket}'/tesis/'\n",
        "exportFilesList = [s for s in filesList if imageFilePrefix in s]\n",
        "# Get the list of image files and the JSON mixer file.\n",
        "imageFilesList = []\n",
        "jsonFile = None\n",
        "for f in exportFilesList:\n",
        "  if f.endswith('.tfrecord.gz'):\n",
        "    imageFilesList.append(f)\n",
        "  elif f.endswith('.json'):\n",
        "    jsonFile = f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'patchDimensions': [128, 128],\n",
            " 'patchesPerRow': 17,\n",
            " 'projection': {'affine': {'doubleMatrix': [10.0,\n",
            "                                            0.0,\n",
            "                                            359880.0,\n",
            "                                            0.0,\n",
            "                                            -10.0,\n",
            "                                            8854970.0]},\n",
            "                'crs': 'EPSG:32718'},\n",
            " 'totalPatches': 238}\n"
          ]
        }
      ],
      "source": [
        "# Load the contents of the mixer file to a JSON object.\n",
        "jsonText = !gsutil cat {jsonFile}\n",
        "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "mixer = json.loads(jsonText.nlstr)\n",
        "pprint(mixer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_input_fn(fileNames,side,bands):\n",
        "\n",
        "  # Read `TFRecordDatasets`\n",
        "  dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n",
        "  featuresDict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}\n",
        "\n",
        "  # Make a parsing function\n",
        "  def parse_image(example_proto):\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
        "    return parsed_features\n",
        "\n",
        "  def stack_images(features):\n",
        "    nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))\n",
        "    return nfeat\n",
        "\n",
        "  dataset = dataset.map(parse_image, num_parallel_calls=4)\n",
        "  dataset = dataset.map(stack_images, num_parallel_calls=4)\n",
        "  dataset = dataset.batch(side*side)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "fileNames = imageFilesList\n",
        "side = 128\n",
        "bands = ['B2', 'B3', 'B4', 'B13', 'B14', 'B15']\n",
        "dataset = tf.data.TFRecordDataset(fileNames[0], compression_type='GZIP')\n",
        "featuresDict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}\n",
        "predict_db = predict_input_fn(fileNames=imageFilesList, side=128, bands=[\"B2\", \"B3\", \"B4\", \"B13\", \"B14\", \"B15\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fileNames = imageFilesList\n",
        "# side = 128\n",
        "\n",
        "# # Read `TFRecordDatasets`\n",
        "# bands = ['B2', 'B3', 'B4', 'B13', 'B14', 'B15']\n",
        "# dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n",
        "# featuresDict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}\n",
        "\n",
        "# # Make a parsing function\n",
        "# def parse_image(example_proto):\n",
        "#     parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
        "#     return parsed_features\n",
        "\n",
        "# def stack_images(features):\n",
        "#     nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))\n",
        "#     return nfeat\n",
        "\n",
        "# dataset = dataset.map(parse_image, num_parallel_calls=4)\n",
        "# dataset = dataset.map(stack_images, num_parallel_calls=4)\n",
        "# dataset = dataset.batch(side*side)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-06 20:58:22.434167: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 6442450944 exceeds 10% of free system memory.\n",
            "2023-07-06 20:58:22.847721: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 249561088 exceeds 10% of free system memory.\n",
            "2023-07-06 20:58:22.893798: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 265402368 exceeds 10% of free system memory.\n",
            "2023-07-06 20:58:24.343346: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 249561088 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 20s 20s/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(predict_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done with patch 1...\n",
            "Done with patch 2...\n",
            "Done with patch 3...\n",
            "Done with patch 4...\n",
            "Done with patch 5...\n",
            "Done with patch 6...\n",
            "Done with patch 7...\n",
            "Done with patch 8...\n",
            "Done with patch 9...\n",
            "Done with patch 10...\n",
            "Done with patch 11...\n",
            "Done with patch 12...\n",
            "Done with patch 13...\n",
            "Done with patch 14...\n",
            "Done with patch 15...\n",
            "Done with patch 16...\n",
            "Done with patch 17...\n",
            "Done with patch 18...\n",
            "Done with patch 19...\n",
            "Done with patch 20...\n",
            "Done with patch 21...\n",
            "Done with patch 22...\n",
            "Done with patch 23...\n",
            "Done with patch 24...\n",
            "Done with patch 25...\n",
            "Done with patch 26...\n",
            "Done with patch 27...\n",
            "Done with patch 28...\n",
            "Done with patch 29...\n",
            "Done with patch 30...\n",
            "Done with patch 31...\n",
            "Done with patch 32...\n",
            "Done with patch 33...\n",
            "Done with patch 34...\n",
            "Done with patch 35...\n",
            "Done with patch 36...\n",
            "Done with patch 37...\n",
            "Done with patch 38...\n",
            "Done with patch 39...\n",
            "Done with patch 40...\n",
            "Done with patch 41...\n",
            "Done with patch 42...\n",
            "Done with patch 43...\n",
            "Done with patch 44...\n",
            "Done with patch 45...\n",
            "Done with patch 46...\n",
            "Done with patch 47...\n",
            "Done with patch 48...\n",
            "Done with patch 49...\n",
            "Done with patch 50...\n",
            "Done with patch 51...\n",
            "Done with patch 52...\n",
            "Done with patch 53...\n",
            "Done with patch 54...\n",
            "Done with patch 55...\n",
            "Done with patch 56...\n",
            "Done with patch 57...\n",
            "Done with patch 58...\n",
            "Done with patch 59...\n",
            "Done with patch 60...\n",
            "Done with patch 61...\n",
            "Done with patch 62...\n",
            "Done with patch 63...\n",
            "Done with patch 64...\n",
            "Done with patch 65...\n",
            "Done with patch 66...\n",
            "Done with patch 67...\n",
            "Done with patch 68...\n",
            "Done with patch 69...\n",
            "Done with patch 70...\n",
            "Done with patch 71...\n",
            "Done with patch 72...\n",
            "Done with patch 73...\n",
            "Done with patch 74...\n",
            "Done with patch 75...\n",
            "Done with patch 76...\n",
            "Done with patch 77...\n",
            "Done with patch 78...\n",
            "Done with patch 79...\n",
            "Done with patch 80...\n",
            "Done with patch 81...\n",
            "Done with patch 82...\n",
            "Done with patch 83...\n",
            "Done with patch 84...\n",
            "Done with patch 85...\n",
            "Done with patch 86...\n",
            "Done with patch 87...\n",
            "Done with patch 88...\n",
            "Done with patch 89...\n",
            "Done with patch 90...\n",
            "Done with patch 91...\n",
            "Done with patch 92...\n",
            "Done with patch 93...\n",
            "Done with patch 94...\n",
            "Done with patch 95...\n",
            "Done with patch 96...\n",
            "Done with patch 97...\n",
            "Done with patch 98...\n",
            "Done with patch 99...\n",
            "Done with patch 100...\n",
            "Done with patch 101...\n",
            "Done with patch 102...\n",
            "Done with patch 103...\n",
            "Done with patch 104...\n",
            "Done with patch 105...\n",
            "Done with patch 106...\n",
            "Done with patch 107...\n",
            "Done with patch 108...\n",
            "Done with patch 109...\n",
            "Done with patch 110...\n",
            "Done with patch 111...\n",
            "Done with patch 112...\n",
            "Done with patch 113...\n",
            "Done with patch 114...\n",
            "Done with patch 115...\n",
            "Done with patch 116...\n",
            "Done with patch 117...\n",
            "Done with patch 118...\n",
            "Done with patch 119...\n",
            "Done with patch 120...\n",
            "Done with patch 121...\n",
            "Done with patch 122...\n",
            "Done with patch 123...\n",
            "Done with patch 124...\n",
            "Done with patch 125...\n",
            "Done with patch 126...\n",
            "Done with patch 127...\n",
            "Done with patch 128...\n",
            "Done with patch 129...\n",
            "Done with patch 130...\n",
            "Done with patch 131...\n",
            "Done with patch 132...\n",
            "Done with patch 133...\n",
            "Done with patch 134...\n",
            "Done with patch 135...\n",
            "Done with patch 136...\n",
            "Done with patch 137...\n",
            "Done with patch 138...\n",
            "Done with patch 139...\n",
            "Done with patch 140...\n",
            "Done with patch 141...\n",
            "Done with patch 142...\n",
            "Done with patch 143...\n",
            "Done with patch 144...\n",
            "Done with patch 145...\n",
            "Done with patch 146...\n",
            "Done with patch 147...\n",
            "Done with patch 148...\n",
            "Done with patch 149...\n",
            "Done with patch 150...\n",
            "Done with patch 151...\n",
            "Done with patch 152...\n",
            "Done with patch 153...\n",
            "Done with patch 154...\n",
            "Done with patch 155...\n",
            "Done with patch 156...\n",
            "Done with patch 157...\n",
            "Done with patch 158...\n",
            "Done with patch 159...\n",
            "Done with patch 160...\n",
            "Done with patch 161...\n",
            "Done with patch 162...\n",
            "Done with patch 163...\n",
            "Done with patch 164...\n",
            "Done with patch 165...\n",
            "Done with patch 166...\n",
            "Done with patch 167...\n",
            "Done with patch 168...\n",
            "Done with patch 169...\n",
            "Done with patch 170...\n",
            "Done with patch 171...\n",
            "Done with patch 172...\n",
            "Done with patch 173...\n",
            "Done with patch 174...\n",
            "Done with patch 175...\n",
            "Done with patch 176...\n",
            "Done with patch 177...\n",
            "Done with patch 178...\n",
            "Done with patch 179...\n",
            "Done with patch 180...\n",
            "Done with patch 181...\n",
            "Done with patch 182...\n",
            "Done with patch 183...\n",
            "Done with patch 184...\n",
            "Done with patch 185...\n",
            "Done with patch 186...\n",
            "Done with patch 187...\n",
            "Done with patch 188...\n",
            "Done with patch 189...\n",
            "Done with patch 190...\n",
            "Done with patch 191...\n",
            "Done with patch 192...\n",
            "Done with patch 193...\n",
            "Done with patch 194...\n",
            "Done with patch 195...\n",
            "Done with patch 196...\n",
            "Done with patch 197...\n",
            "Done with patch 198...\n",
            "Done with patch 199...\n",
            "Done with patch 200...\n",
            "Done with patch 201...\n",
            "Done with patch 202...\n",
            "Done with patch 203...\n",
            "Done with patch 204...\n",
            "Done with patch 205...\n",
            "Done with patch 206...\n",
            "Done with patch 207...\n",
            "Done with patch 208...\n",
            "Done with patch 209...\n",
            "Done with patch 210...\n",
            "Done with patch 211...\n",
            "Done with patch 212...\n",
            "Done with patch 213...\n",
            "Done with patch 214...\n",
            "Done with patch 215...\n",
            "Done with patch 216...\n",
            "Done with patch 217...\n",
            "Done with patch 218...\n",
            "Done with patch 219...\n",
            "Done with patch 220...\n",
            "Done with patch 221...\n",
            "Done with patch 222...\n",
            "Done with patch 223...\n",
            "Done with patch 224...\n",
            "Done with patch 225...\n",
            "Done with patch 226...\n",
            "Done with patch 227...\n",
            "Done with patch 228...\n",
            "Done with patch 229...\n",
            "Done with patch 230...\n",
            "Done with patch 231...\n",
            "Done with patch 232...\n",
            "Done with patch 233...\n",
            "Done with patch 234...\n",
            "Done with patch 235...\n",
            "Done with patch 236...\n",
            "Done with patch 237...\n",
            "Done with patch 238...\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the writer.\n",
        "PATCH_WIDTH , PATCH_HEIGHT = [128,128]\n",
        "outputImageFile = 'gs://' + outputBucket + '/tesis/ld_out.TFRecord'\n",
        "writer = tf.io.TFRecordWriter(outputImageFile)\n",
        "curPatch = 1\n",
        "for  prediction in predictions:\n",
        "  patch = prediction.squeeze().T.flatten().tolist()\n",
        "\n",
        "  if (len(patch) == PATCH_WIDTH * PATCH_HEIGHT):\n",
        "    print('Done with patch ' + str(curPatch) + '...')\n",
        "    # Create an example\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'crop_prob': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=patch))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "\n",
        "    writer.write(example.SerializeToString())\n",
        "    curPatch += 1\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_input_fn(fileNames, side, bands):\n",
        "\n",
        "  # Read `TFRecordDatasets`\n",
        "  dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')\n",
        "  featuresDict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}\n",
        "\n",
        "  # Make a parsing function\n",
        "  def parse_image(example_proto):\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
        "    return parsed_features\n",
        "\n",
        "  def stack_images(features):\n",
        "    nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))\n",
        "    return nfeat\n",
        "\n",
        "  dataset = dataset.map(parse_image, num_parallel_calls=4)\n",
        "  dataset = dataset.map(stack_images, num_parallel_calls=4)\n",
        "  dataset = dataset.batch(side*side)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def list_blobs(bucket_name, prefix=None):\n",
        "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix=prefix)\n",
        "    return [blob.name for blob in blobs]\n",
        "\n",
        "imageFilePrefix = 'tesis/ld'\n",
        "filesList = list_blobs('rgee_dev', imageFilePrefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tesis/ld.json', 'tesis/ld.tfrecord.gz', 'tesis/ld_out.TFRecord']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filesList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "bucket_name = 'rgee_dev'\n",
        "folder_name = 'tesis'\n",
        "exportFilesList = [f\"gs://{bucket_name}/{s}\" for s in filesList if imageFilePrefix in s]\n",
        "# Get the list of image files and the JSON mixer file.\n",
        "imageFilesList = []\n",
        "jsonFile = None\n",
        "for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "        imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "        jsonFile = f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['gs://rgee_dev/tesis/ld.tfrecord.gz']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imageFilesList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_text = None\n",
        "jsonFile = 'gs://rgee_dev/tesis/ld.json'\n",
        "with tf.io.gfile.GFile(jsonFile, 'r') as f:\n",
        "    json_text = f.read()\n",
        "    mixer = json.loads(json_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[10.0, 0.0, 359880.0, 0.0, -10.0, 8854970.0]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mixer[\"projection\"][\"affine\"][\"doubleMatrix\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"../models/model_resnet34_6b_ns.json\", \"r\") as json_file:\n",
        "    json_config = json_file.read()\n",
        "model = tf.keras.models.model_from_json(json_config)\n",
        "model.load_weights(\"../models/model_resnet34_6b_ns.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "fileNames = imageFilesList\n",
        "side = 128\n",
        "bands = ['B2', 'B3', 'B4', 'B13', 'B14', 'B15']\n",
        "predict_db = predict_input_fn(fileNames=imageFilesList, side=128, bands=[\"B2\", \"B3\", \"B4\", \"B13\", \"B14\", \"B15\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 31s 31s/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(predict_db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  15608754  2023-07-02T15:39:32Z  gs://rgee_dev/tesis/ld_out.TFRecord\n",
            "TOTAL: 1 objects, 15608754 bytes (14.89 MiB)\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls -l {outputImageFile}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing to users/ryali93/ld_out\n"
          ]
        }
      ],
      "source": [
        "# REPLACE WITH YOUR USERNAME:\n",
        "USER_NAME = 'ryali93'\n",
        "outputAssetID = 'users/' + USER_NAME + '/ld_out'\n",
        "print('Writing to ' + outputAssetID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started upload task with ID: LI3VG7KQVKMCQ2H4IXJSR2WN\n"
          ]
        }
      ],
      "source": [
        "# Start the upload. It step might take a while.\n",
        "!earthengine upload image --asset_id={outputAssetID} {outputImageFile} {jsonFile}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'projection': {'crs': 'EPSG:32718',\n",
              "  'affine': {'doubleMatrix': [10.0, 0.0, 359880.0, 0.0, -10.0, 8854970.0]}},\n",
              " 'patchDimensions': [128, 128],\n",
              " 'patchesPerRow': 17,\n",
              " 'totalPatches': 238}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing to users/ryali93/ld_out_predictions\n",
            "Done with patch 1...\n",
            "Done with patch 2...\n",
            "Done with patch 3...\n",
            "Done with patch 4...\n",
            "Done with patch 5...\n",
            "Done with patch 6...\n",
            "Done with patch 7...\n",
            "Done with patch 8...\n",
            "Done with patch 9...\n",
            "Done with patch 10...\n",
            "Done with patch 11...\n",
            "Done with patch 12...\n",
            "Done with patch 13...\n",
            "Done with patch 14...\n",
            "Done with patch 15...\n",
            "Done with patch 16...\n",
            "Done with patch 17...\n",
            "Done with patch 18...\n",
            "Done with patch 19...\n",
            "Done with patch 20...\n",
            "Done with patch 21...\n",
            "Done with patch 22...\n",
            "Done with patch 23...\n",
            "Done with patch 24...\n",
            "Done with patch 25...\n",
            "Done with patch 26...\n",
            "Done with patch 27...\n",
            "Done with patch 28...\n",
            "Done with patch 29...\n",
            "Done with patch 30...\n",
            "Done with patch 31...\n",
            "Done with patch 32...\n",
            "Done with patch 33...\n",
            "Done with patch 34...\n",
            "Done with patch 35...\n",
            "Done with patch 36...\n",
            "Done with patch 37...\n",
            "Done with patch 38...\n",
            "Done with patch 39...\n",
            "Done with patch 40...\n",
            "Done with patch 41...\n",
            "Done with patch 42...\n",
            "Done with patch 43...\n",
            "Done with patch 44...\n",
            "Done with patch 45...\n",
            "Done with patch 46...\n",
            "Done with patch 47...\n",
            "Done with patch 48...\n",
            "Done with patch 49...\n",
            "Done with patch 50...\n",
            "Done with patch 51...\n",
            "Done with patch 52...\n",
            "Done with patch 53...\n",
            "Done with patch 54...\n",
            "Done with patch 55...\n",
            "Done with patch 56...\n",
            "Done with patch 57...\n",
            "Done with patch 58...\n",
            "Done with patch 59...\n",
            "Done with patch 60...\n",
            "Done with patch 61...\n",
            "Done with patch 62...\n",
            "Done with patch 63...\n",
            "Done with patch 64...\n",
            "Done with patch 65...\n",
            "Done with patch 66...\n",
            "Done with patch 67...\n",
            "Done with patch 68...\n",
            "Done with patch 69...\n",
            "Done with patch 70...\n",
            "Done with patch 71...\n",
            "Done with patch 72...\n",
            "Done with patch 73...\n",
            "Done with patch 74...\n",
            "Done with patch 75...\n",
            "Done with patch 76...\n",
            "Done with patch 77...\n",
            "Done with patch 78...\n",
            "Done with patch 79...\n",
            "Done with patch 80...\n",
            "Done with patch 81...\n",
            "Done with patch 82...\n",
            "Done with patch 83...\n",
            "Done with patch 84...\n",
            "Done with patch 85...\n",
            "Done with patch 86...\n",
            "Done with patch 87...\n",
            "Done with patch 88...\n",
            "Done with patch 89...\n",
            "Done with patch 90...\n",
            "Done with patch 91...\n",
            "Done with patch 92...\n",
            "Done with patch 93...\n",
            "Done with patch 94...\n",
            "Done with patch 95...\n",
            "Done with patch 96...\n",
            "Done with patch 97...\n",
            "Done with patch 98...\n",
            "Done with patch 99...\n",
            "Done with patch 100...\n",
            "Done with patch 101...\n",
            "Done with patch 102...\n",
            "Done with patch 103...\n",
            "Done with patch 104...\n",
            "Done with patch 105...\n",
            "Done with patch 106...\n",
            "Done with patch 107...\n",
            "Done with patch 108...\n",
            "Done with patch 109...\n",
            "Done with patch 110...\n",
            "Done with patch 111...\n",
            "Done with patch 112...\n",
            "Done with patch 113...\n",
            "Done with patch 114...\n",
            "Done with patch 115...\n",
            "Done with patch 116...\n",
            "Done with patch 117...\n",
            "Done with patch 118...\n",
            "Done with patch 119...\n",
            "Done with patch 120...\n",
            "Done with patch 121...\n",
            "Done with patch 122...\n",
            "Done with patch 123...\n",
            "Done with patch 124...\n",
            "Done with patch 125...\n",
            "Done with patch 126...\n",
            "Done with patch 127...\n",
            "Done with patch 128...\n",
            "Done with patch 129...\n",
            "Done with patch 130...\n",
            "Done with patch 131...\n",
            "Done with patch 132...\n",
            "Done with patch 133...\n",
            "Done with patch 134...\n",
            "Done with patch 135...\n",
            "Done with patch 136...\n",
            "Done with patch 137...\n",
            "Done with patch 138...\n",
            "Done with patch 139...\n",
            "Done with patch 140...\n",
            "Done with patch 141...\n",
            "Done with patch 142...\n",
            "Done with patch 143...\n",
            "Done with patch 144...\n",
            "Done with patch 145...\n",
            "Done with patch 146...\n",
            "Done with patch 147...\n",
            "Done with patch 148...\n",
            "Done with patch 149...\n",
            "Done with patch 150...\n",
            "Done with patch 151...\n",
            "Done with patch 152...\n",
            "Done with patch 153...\n",
            "Done with patch 154...\n",
            "Done with patch 155...\n",
            "Done with patch 156...\n",
            "Done with patch 157...\n",
            "Done with patch 158...\n",
            "Done with patch 159...\n",
            "Done with patch 160...\n",
            "Done with patch 161...\n",
            "Done with patch 162...\n",
            "Done with patch 163...\n",
            "Done with patch 164...\n",
            "Done with patch 165...\n",
            "Done with patch 166...\n",
            "Done with patch 167...\n",
            "Done with patch 168...\n",
            "Done with patch 169...\n",
            "Done with patch 170...\n",
            "Done with patch 171...\n",
            "Done with patch 172...\n",
            "Done with patch 173...\n",
            "Done with patch 174...\n",
            "Done with patch 175...\n",
            "Done with patch 176...\n",
            "Done with patch 177...\n",
            "Done with patch 178...\n",
            "Done with patch 179...\n",
            "Done with patch 180...\n",
            "Done with patch 181...\n",
            "Done with patch 182...\n",
            "Done with patch 183...\n",
            "Done with patch 184...\n",
            "Done with patch 185...\n",
            "Done with patch 186...\n",
            "Done with patch 187...\n",
            "Done with patch 188...\n",
            "Done with patch 189...\n",
            "Done with patch 190...\n",
            "Done with patch 191...\n",
            "Done with patch 192...\n",
            "Done with patch 193...\n",
            "Done with patch 194...\n",
            "Done with patch 195...\n",
            "Done with patch 196...\n",
            "Done with patch 197...\n",
            "Done with patch 198...\n",
            "Done with patch 199...\n",
            "Done with patch 200...\n",
            "Done with patch 201...\n",
            "Done with patch 202...\n",
            "Done with patch 203...\n",
            "Done with patch 204...\n",
            "Done with patch 205...\n",
            "Done with patch 206...\n",
            "Done with patch 207...\n",
            "Done with patch 208...\n",
            "Done with patch 209...\n",
            "Done with patch 210...\n",
            "Done with patch 211...\n",
            "Done with patch 212...\n",
            "Done with patch 213...\n",
            "Done with patch 214...\n",
            "Done with patch 215...\n",
            "Done with patch 216...\n",
            "Done with patch 217...\n",
            "Done with patch 218...\n",
            "Done with patch 219...\n",
            "Done with patch 220...\n",
            "Done with patch 221...\n",
            "Done with patch 222...\n",
            "Done with patch 223...\n",
            "Done with patch 224...\n",
            "Done with patch 225...\n",
            "Done with patch 226...\n",
            "Done with patch 227...\n",
            "Done with patch 228...\n",
            "Done with patch 229...\n",
            "Done with patch 230...\n",
            "Done with patch 231...\n",
            "Done with patch 232...\n",
            "Done with patch 233...\n",
            "Done with patch 234...\n",
            "Done with patch 235...\n",
            "Done with patch 236...\n",
            "Done with patch 237...\n",
            "Done with patch 238...\n"
          ]
        }
      ],
      "source": [
        "USER_NAME = 'ryali93'\n",
        "NAME_OUT = 'ld_out_predictions'\n",
        "outputAssetID = f'users/{USER_NAME}/{NAME_OUT}'\n",
        "print('Writing to ' + outputAssetID)\n",
        "# Instantiate the writer.\n",
        "PATCH_WIDTH , PATCH_HEIGHT = [128,128]\n",
        "outputImageFile = f'gs://{bucket_name}/{folder_name}/{NAME_OUT}_test.TFRecord'\n",
        "writer = tf.io.TFRecordWriter(outputImageFile)\n",
        "curPatch = 1\n",
        "for  prediction in predictions:\n",
        "    patch = prediction.squeeze().T.flatten().tolist()\n",
        "\n",
        "    if (len(patch) == PATCH_WIDTH * PATCH_HEIGHT):\n",
        "        print('Done with patch ' + str(curPatch) + '...')\n",
        "        # Create an example\n",
        "        example = tf.train.Example(\n",
        "            features=tf.train.Features(\n",
        "            feature={\n",
        "                'ld_prob': \n",
        "                tf.train.Feature(\n",
        "                    float_list=tf.train.FloatList(value=patch))\n",
        "            })\n",
        "        )\n",
        "\n",
        "    writer.write(example.SerializeToString())\n",
        "    curPatch += 1\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "ename": "EEException",
          "evalue": "Invalid JSON payload received. Unknown name \"name\" at 'image_manifest': Proto field is not repeating, cannot start list.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/Desktop/landslide4sense2022/venv/lib/python3.10/site-packages/ee/data.py:345\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m   \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49mexecute(num_retries\u001b[39m=\u001b[39;49mnum_retries)\n\u001b[1;32m    346\u001b[0m \u001b[39mexcept\u001b[39;00m googleapiclient\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mHttpError \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Desktop/landslide4sense2022/venv/lib/python3.10/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         logger\u001b[39m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Desktop/landslide4sense2022/venv/lib/python3.10/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m HttpError(resp, content, uri\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostproc(resp, content)\n",
            "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/image:import?alt=json returned \"Invalid JSON payload received. Unknown name \"name\" at 'image_manifest': Proto field is not repeating, cannot start list.\". Details: \"[{'@type': 'type.googleapis.com/google.rpc.BadRequest', 'fieldViolations': [{'field': 'image_manifest', 'description': 'Invalid JSON payload received. Unknown name \"name\" at \\'image_manifest\\': Proto field is not repeating, cannot start list.'}]}]\">",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mEEException\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[66], line 20\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# asset_request = {\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# 'id': outputAssetId,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# 'tilesets': [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# 'affine': mixer[\"projection\"][\"affine\"][\"doubleMatrix\"],\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# }\u001b[39;00m\n\u001b[1;32m     16\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: outputAssetId,\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtilesets\u001b[39m\u001b[39m\"\u001b[39m: [{\u001b[39m\"\u001b[39m\u001b[39msources\u001b[39m\u001b[39m\"\u001b[39m: [{\u001b[39m\"\u001b[39m\u001b[39muris\u001b[39m\u001b[39m\"\u001b[39m: [outputImageFile]}]}]\n\u001b[1;32m     19\u001b[0m }\n\u001b[0;32m---> 20\u001b[0m ee\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mstartIngestion(request_id\u001b[39m=\u001b[39;49mee\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mnewTaskId()[\u001b[39m0\u001b[39;49m], params\u001b[39m=\u001b[39;49mparams)\n",
            "File \u001b[0;32m~/Desktop/landslide4sense2022/venv/lib/python3.10/site-packages/ee/data.py:1726\u001b[0m, in \u001b[0;36mstartIngestion\u001b[0;34m(request_id, params, allow_overwrite)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[39m# It's only safe to retry the request if there's a unique ID to make it\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[39m# idempotent.\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m num_retries \u001b[39m=\u001b[39m MAX_RETRIES \u001b[39mif\u001b[39;00m request_id \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1726\u001b[0m operation \u001b[39m=\u001b[39m _execute_cloud_call(\n\u001b[1;32m   1727\u001b[0m     _get_cloud_projects()\n\u001b[1;32m   1728\u001b[0m     \u001b[39m.\u001b[39;49mimage()\n\u001b[1;32m   1729\u001b[0m     \u001b[39m.\u001b[39;49mimport_(project\u001b[39m=\u001b[39;49m_get_projects_path(), body\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m   1730\u001b[0m     num_retries\u001b[39m=\u001b[39;49mnum_retries,\n\u001b[1;32m   1731\u001b[0m )\n\u001b[1;32m   1732\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m   1733\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1734\u001b[0m         _cloud_api_utils\u001b[39m.\u001b[39mconvert_operation_name_to_task_id(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstarted\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mOK\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1738\u001b[0m }\n",
            "File \u001b[0;32m~/Desktop/landslide4sense2022/venv/lib/python3.10/site-packages/ee/data.py:347\u001b[0m, in \u001b[0;36m_execute_cloud_call\u001b[0;34m(call, num_retries)\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39mexecute(num_retries\u001b[39m=\u001b[39mnum_retries)\n\u001b[1;32m    346\u001b[0m \u001b[39mexcept\u001b[39;00m googleapiclient\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mHttpError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 347\u001b[0m   \u001b[39mraise\u001b[39;00m _translate_cloud_exception(e)\n",
            "\u001b[0;31mEEException\u001b[0m: Invalid JSON payload received. Unknown name \"name\" at 'image_manifest': Proto field is not repeating, cannot start list."
          ]
        }
      ],
      "source": [
        "import ee\n",
        "ee.Initialize()\n",
        "# outputAssetId = 'users/ryali93/ld_out_predictions'\n",
        "outputAssetId = \"projects/earthengine-legacy/assets/users/ryali93/ld_out_predictions\",\n",
        "gcs_image_uri = outputImageFile\n",
        "# asset_request = {\n",
        "# 'id': outputAssetId,\n",
        "# 'tilesets': [\n",
        "#     {\n",
        "#     'sources': [{'primaryPath': gcs_image_uri}]\n",
        "#     }\n",
        "# ],\n",
        "# 'crs': mixer[\"projection\"][\"crs\"],\n",
        "# 'affine': mixer[\"projection\"][\"affine\"][\"doubleMatrix\"],\n",
        "# }\n",
        "params = {\n",
        "    \"name\": outputAssetId,\n",
        "    \"tilesets\": [{\"sources\": [{\"uris\": [outputImageFile]}]}]\n",
        "}\n",
        "ee.data.startIngestion(request_id=ee.data.newTaskId()[0], params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: earthengine upload image [-h] [--wait [WAIT]] [--force]\n",
            "                                [--asset_id ASSET_ID] [--last_band_alpha]\n",
            "                                [--nodata_value NODATA_VALUE]\n",
            "                                [--pyramiding_policy PYRAMIDING_POLICY]\n",
            "                                [--bands BANDS] [--crs CRS]\n",
            "                                [--manifest MANIFEST] [--property PROPERTY]\n",
            "                                [--time_start TIME_START]\n",
            "                                [--time_end TIME_END]\n",
            "                                [src_files ...]\n",
            "\n",
            "Uploads an image from Cloud Storage to Earth Engine. See docs for \"asset set\"\n",
            "for additional details on how to specify asset metadata properties.\n",
            "\n",
            "positional arguments:\n",
            "  src_files             Cloud Storage URL(s) of the file(s) to upload. Must\n",
            "                        have the prefix 'gs://'.\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --wait [WAIT], -w [WAIT]\n",
            "                        Wait for the task to finish, or timeout after the\n",
            "                        specified number of seconds. Without this flag, the\n",
            "                        command just starts an export task in the background,\n",
            "                        and returns immediately.\n",
            "  --force, -f           Overwrite any existing version of the asset.\n",
            "  --asset_id ASSET_ID   Destination asset ID for the uploaded file.\n",
            "  --last_band_alpha     Use the last band as a masking channel for all bands.\n",
            "                        Mutually exclusive with nodata_value.\n",
            "  --nodata_value NODATA_VALUE\n",
            "                        Value for missing data. Mutually exclusive with\n",
            "                        last_band_alpha.\n",
            "  --pyramiding_policy PYRAMIDING_POLICY\n",
            "                        The pyramid reduction policy to use\n",
            "  --bands BANDS         Comma-separated list of names to use for the image\n",
            "                        bands.\n",
            "  --crs CRS             The coordinate reference system, to override the map\n",
            "                        projection of the image. May be either a well-known\n",
            "                        authority code (e.g. EPSG:4326) or a WKT string.\n",
            "  --manifest MANIFEST   Local path to a JSON asset manifest file. No other\n",
            "                        flags are used if this flag is set.\n",
            "  --property PROPERTY, -p PROPERTY\n",
            "                        A property to set, in the form [(type)]name=value. If\n",
            "                        no type is specified the type will be \"number\" if the\n",
            "                        value is numeric and \"string\" otherwise. May be\n",
            "                        provided multiple times.\n",
            "  --time_start TIME_START, -ts TIME_START\n",
            "                        Sets the start time property to a number or date.\n",
            "  --time_end TIME_END, -te TIME_END\n",
            "                        Sets the end time property to a number or date.\n"
          ]
        }
      ],
      "source": [
        "!earthengine upload image --asset_id={outputAssetID} {outputImageFile} {jsonFile}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "f759adf144e8b7ad74c47998e548c93270d038d4b7f90e643deae1f4a2c5b4b8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
